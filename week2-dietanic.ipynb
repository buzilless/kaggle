{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA To Prediction(DidTanic)","metadata":{}},{"cell_type":"markdown","source":"Sometimes life has a cruel sense of humor, giving you the thing you always wanted at the worst time possible.\n때때로 삶은 가능한 최악의 시간에 항상 원했던 것을 당신에게 주는 잔인한 유머 감각을 가지고 있다.","metadata":{}},{"cell_type":"markdown","source":"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n타이타닉의 침몰은 역사에서 가장 악명높은 난파사고중 하나입니다. 1912.4.15 처녀 항해중, 타이타닉은 빙산과 충돌후 침몰했다 2224명의 선원과 승무원중 1502명이 사망했다. 이것이 '다이타닉' 이름이 붙은 이유이다. 이것은 전세계의 누구도 잊을수 없는 불운한 재앙이다\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n타이타닉을 짓기 위해 750만불이 들었고 충돌 때문에 바다 아래로 가라앉았다.\n타이타닉 데이터셋은 초심자가 데이터 과학자로써 시작하기에, 캐글 경쟁에 참가하기에 가장 좋다\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n이 노트북의 목적은 어떤 예측적인 모델링 문제에서 어떻게 작업흐름을 가질지에 대한 목적이다 \n어떻게 우리는 특징들을 확인할수 있을까, 어떻게 우리는 새로운 피쳐와 몇몇 머신러닝 컨셉에 추가할수 있을까, 나는 가능한 기본적인 노트북을 유지하기 위해 노력했다 뉴비들도 모든 단계를 이해할 수 있기 위해\nIf You Like the notebook and think that it helped you..PLEASE UPVOTE. It will keep me motivated.\n만약 니가 노트북이 좋고 너에게 도움이 됐다고 생각한다면 제발 투표해줘 이건 나의 동기부여가 될꺼야!\n","metadata":{}},{"cell_type":"markdown","source":"# Contents of the Notebook(노트의 내용)\n\n### Part1: Exploratory Data Analysis(EDA)(탐색적 데이터 분석)\n- Analysis of the features(특징들의 분석)\n- Finding any realations or trends considering multiple features.(다양한 특징들을 고려한 경향이나 어떤 관계들을 찾는것)\n\n### Part2: Feature Engineering and Data Cleaning(데이터 정리와 피쳐 엔지니어링)\n- Adding any few features. (특징들 추가)\n- Removing redundant features. (중복기능 제거)\n- Converting features into suitable form for modeling.  (모델링에 적합한 형태로 특징들을 변환)\n\n### Part3: Predictive Modeling (에측 모델링)\n- Running Basic Algorithms.(기본 알고리즘 동작)\n- Cross Validation.(교차 검증) \n- Ensembling.(앙상블 : 전체적 조화)  (아주중요)\n- Important Features Extraction. (중요 기능 추출)\n","metadata":{}},{"cell_type":"markdown","source":"# Part1: Exploratory Data Analysis(EDA)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')  # ?\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:38.737249Z","iopub.execute_input":"2022-07-22T01:30:38.738392Z","iopub.status.idle":"2022-07-22T01:30:40.027955Z","shell.execute_reply.started":"2022-07-22T01:30:38.738260Z","shell.execute_reply":"2022-07-22T01:30:40.026738Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"- 라이브러리 호출","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.030119Z","iopub.execute_input":"2022-07-22T01:30:40.030464Z","iopub.status.idle":"2022-07-22T01:30:40.053128Z","shell.execute_reply.started":"2022-07-22T01:30:40.030433Z","shell.execute_reply":"2022-07-22T01:30:40.051848Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"- 데이터셋 호출","metadata":{}},{"cell_type":"code","source":"data.head()  \n# 데이터 5행까지 보기 ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.055187Z","iopub.execute_input":"2022-07-22T01:30:40.055629Z","iopub.status.idle":"2022-07-22T01:30:40.083056Z","shell.execute_reply.started":"2022-07-22T01:30:40.055584Z","shell.execute_reply":"2022-07-22T01:30:40.081836Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### why?\n- head를 통해 간략하게 데이터와 칼럼들을 확인한다 (어떤 칼럼이 있는지 쳌)","metadata":{}},{"cell_type":"markdown","source":"## 첫번째로 해야하는 Null값 체크","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()  \n# 칼럼별 결측값 개수 \n# 데이터 안의 null값 총합\n\n# 참고: 결측값의 유무를 확인 할 때는 isnull 뿐만아니라 isna() 도 가능하\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.086259Z","iopub.execute_input":"2022-07-22T01:30:40.086886Z","iopub.status.idle":"2022-07-22T01:30:40.098296Z","shell.execute_reply.started":"2022-07-22T01:30:40.086839Z","shell.execute_reply":"2022-07-22T01:30:40.097022Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### sum 등의 함수 정리 필요!!!","metadata":{}},{"cell_type":"markdown","source":"- the Age,Cabin and Embarked have null values. I will try to fix them\n- (나이, 항구에서 널값이 확인되었다 이것을 수정할것이다)\n\n### 널데이터를 채우는것 == fix라고 한다","metadata":{}},{"cell_type":"markdown","source":"## How many Survived??","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize = (18,8))   # 1행2열로 표현 사이즈는 18,8인치\ndata['Survived'].value_counts().plot.pie(explode = [0,0.1],autopct='%1.1f%%',ax=ax[0],shadow =True)\n# survived 구간의 갯수를 카운트 해서 pie차트로 표현 0,0.1간격을 띄우고 각 파이에 %수치 소수점1까지 표현, 1열에 위치하고 그림자 있음\n# value_count는 갯수를 세주지만 null값은 포함하지 않는다\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])  # Survived 의 수를 세어 2열에 표시\nax[1].set_title('Survived')   # 1열에 타이틀 설정\nplt.show()\n# 얼마나 살았는지 pie, bar모양으로 시각화 해서 확인","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.100422Z","iopub.execute_input":"2022-07-22T01:30:40.101163Z","iopub.status.idle":"2022-07-22T01:30:40.454619Z","shell.execute_reply.started":"2022-07-22T01:30:40.101118Z","shell.execute_reply":"2022-07-22T01:30:40.453273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"It is evident that not many passengers survived the accident.     \n많은 승객들이 사고를 생존하지 못했다는 것은 명백하다      \n\nOut of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.   \n트레이닝 셋에서 891명의 승객중, 오직 350명 정도가 살아남았다.  오직 38.4% 만이 살아남았다. 우리는 데이터로부터 더 많은 insight를 얻기위해 더 깊이 파고갈 필요가 있고 어떤 범주의 승객들이 살아남았는지 그리고 누가 살아남지 못했는지를 봐야 한다.      \n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.     \n**우리는 데이터셋의 다른특징들을 사용해 생존율을 확인 할것이다. 성별 항구 나이등등에 관한 몇몇 특징들**\n     \nFirst let us understand the different types of features      \n먼저 다른 유형의 feature들을 이해하자","metadata":{}},{"cell_type":"markdown","source":"## Type Of Features\n\n- Categorical Features:( 순서에 무관해서 정렬 할 수 없다 그냥 분류만 한 \nA categorical variable is one that has two or more categories and each value in that feature can be categoriesd by them. For example, gender is a categorical variable having two categories(male and female). Now we cannot sort or give any ordering th such variables. They are also known as Norminal Variables.      \n범주형 변수는 두개이상의 범주가 있는 변수이며 해당 범주의 각 값은 범주로 분류 될 수 있습니다. \n예를 들어, 성별은 두개의 카테고리를 가지는 범주형 변수이다\n이제 우리는 주어진 변수들을 정리하거나 순서를 지정해 줄수 없습니다. 그들은 정규 변수로서 알려져 있습니다\n=> 즉 범주형 변수에는 순서는 없다\n\n- Categorical Features in the dataset: Sex,Embarked.(데이터셋에서 카테고리별 특징들 : 성별, 항구)\n        \n- Ordinal Features:(순서특징)        \nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable.        \n순서형변수는 범주형 변수와 비슷하다, 그러나 차이점은 값사이에 상대적 순서 또는 정렬을 가질수 있다는 점 입니다        \n만약 우리가 큰, 중간, 작은 과같은 값을 갖는 높이라는 특징이 있다면, 높이는 순서형 범주이다 여기서 우리는 변수에 상대적인 정렬을 할 수 있다        \n=> 순서형 범주는 순서가 있다\n(추후에 encoding할때 순서가 있으면 label encoding만 하면 되지만\n순서가 없을 경우 label encoding을 한 후 one hot encoding/frequency encoding/ mean encoding을 해준다)        \n- Ordinal Features in the dataset: PClass (데이터셋에서의 순서형 범주: Pclass)\n        \n- Continous Feature:  (연속적인 변수:)        \nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.        \n두 점 사이 또는 형상 열의 최소값 또는 최대값 사이의 갑을 취할 수 있는 형상을 연속형이라 한      \n- Continous Features in the dataset: Age","metadata":{}},{"cell_type":"markdown","source":"## Analysing The Features\n본격적인 데이터 분석 시작!\n\n## Sex -> Categorial Feature\n성별 -> 범주형","metadata":{}},{"cell_type":"code","source":"data.groupby(['Sex','Survived'])['Survived'].count() \n# 넣어준 피쳐에 대해 그룹을 묶어 준\n# 성별과 생존율에 대한 데이터를 보기 위해 groupby를 이용해 성별별로 생존자를 파악한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.456346Z","iopub.execute_input":"2022-07-22T01:30:40.456822Z","iopub.status.idle":"2022-07-22T01:30:40.469648Z","shell.execute_reply.started":"2022-07-22T01:30:40.456774Z","shell.execute_reply":"2022-07-22T01:30:40.468661Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize = (18,8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])  #hue 는 색깔 나누\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()\n# 위에서 묶은 그룹의 수치를 insight를 얻기 위해 subplot으로 시각화 한다\n# 남성보다 여성의 생존율이 훨씬 높은것을 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.471793Z","iopub.execute_input":"2022-07-22T01:30:40.472506Z","iopub.status.idle":"2022-07-22T01:30:40.806303Z","shell.execute_reply.started":"2022-07-22T01:30:40.472462Z","shell.execute_reply":"2022-07-22T01:30:40.805127Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%.\n흥미롭다. 배에서 남자의 수가 여자보다 많다. 남자보다 여자의 생존율이 더 높다\nThis looks to be a very important feature for modeling. But is it the best?? Lets check other features.\n이것은 매우 중요한 피쳐로 보인다. 그러나 이게 베스트인가? 다른것도 찾아보자!\n\n### 성별 : 남성보다 여성의 생존율이 더 높다!","metadata":{}},{"cell_type":"markdown","source":"## Pclass -> Ordinal Feature","metadata":{}},{"cell_type":"markdown","source":"Pclass에 대한 feature를 찾아보자!","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')\n# 크로스탭을 사용해서 Pclass 와 Survived 데이터를 정렬한다\n# margin은 총합을 볼지 안볼지 정하는 것이고 background메서드를 이용해 값별로 색깔을 달리해 눈에 잘띄게 해준다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.807978Z","iopub.execute_input":"2022-07-22T01:30:40.808329Z","iopub.status.idle":"2022-07-22T01:30:40.935767Z","shell.execute_reply.started":"2022-07-22T01:30:40.808297Z","shell.execute_reply":"2022-07-22T01:30:40.934873Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass: Survived vs Dead')\nplt.show()\n# 위의 표를 더 잘 보기 위해 barplot으로 본다\n# 왼쪽 그래프를 보니 3클래스의 인원이 가장 많은 것을 확인 할 수있다\n# 오른쪽 그래프를 보니 3클래스의 생존율이 가장 낮을 것을 알 수 있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:40.937348Z","iopub.execute_input":"2022-07-22T01:30:40.937941Z","iopub.status.idle":"2022-07-22T01:30:41.299227Z","shell.execute_reply.started":"2022-07-22T01:30:40.937904Z","shell.execute_reply":"2022-07-22T01:30:41.297963Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"People say Money Can't Buy Everything. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%.     \n사람들은 돈으로 모든것을 살수없다고 한다. 하지만 우리는 명확하게 볼수 있다 1class 승객이 매우 우선적인 구조를 받았다는걸. 비록 3클래스에 승객의 수가 많이 높긴하지만 여전히 생존율은 매우낮다 25%정도로\n\nFor Pclass 1 %survived is around 63% while for Pclass2 is around 48%. So money and status matters. Such a materialistic world.     \n1클래스 생존율은 63%. 반면에 2클래스는 48%정도 이다 돈과 지위가 중요하다 . 물질만능주의 세\nLets Dive in little bit more and check for other interesting observations. Lets check survival rate with Sex and Pclass Together.     \n좀더 빠져보자 그리고 확인해보자 다른 흥미로운 관점을 이제 성별과 Pclass의 생존비율을 살펴보자","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins = True).style.background_gradient(cmap='summer_r')\n# crosstab을 이용해 성별과 생존율을 pclass에 관하여 표로 정리해 본다\n# 남성의 사망과 여성의 생존쪽 색이 진한걸로 여성은 생존율이 높고 남성은 생존율이 낮은걸 한번더 확인 할 수 있다\n# 남성의 사망율중 3class에 색이 가장 진한걸 봐선 3클래스의 남성은 대부분이 사망하였다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:41.305380Z","iopub.execute_input":"2022-07-22T01:30:41.306810Z","iopub.status.idle":"2022-07-22T01:30:41.388351Z","shell.execute_reply.started":"2022-07-22T01:30:41.306733Z","shell.execute_reply":"2022-07-22T01:30:41.386988Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',data=data)\nplt.show()\n# 위의 결과 값을 factorplot을 이용해 눈에 띄게 시각화 한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:41.390331Z","iopub.execute_input":"2022-07-22T01:30:41.391217Z","iopub.status.idle":"2022-07-22T01:30:41.999914Z","shell.execute_reply.started":"2022-07-22T01:30:41.391165Z","shell.execute_reply":"2022-07-22T01:30:41.998592Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 지금까지 범주형(생존)과 순서형(클래스)을 정리하고 시각화 하여 특징들을 알아보았다\n### 이제는 연속적인 값들을 정리하고 시각화 하겠다.","metadata":{}},{"cell_type":"markdown","source":"We use FactorPlot in this case, because they make the seperation of categorical values easy.\n우리는 이경우에 factorplot을 쓸 수 있다. 이거는 쉽게 카테고리별 값을 분리해준다\n\nLooking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\nfactorplot과 크로스탭으로 보면, 우리는 쉽게 1클래스에 여성의 생존율이 95-96%인것을 참조할수 있다 오직 94명중 3명만 사망했다\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\npclass와 관계없이 여성이 구조에 우선순위를 갖는다는것이 명백하다 비록 1클래스의 남성들이 매우 낮은 생존율을 갖지\n\nLooks like Pclass is also an important feature. Lets analyse other features.\n보이는것 처럼 pclass 또한 중요한 특징이다 다른 피쳐도 살펴보도록 하자!","metadata":{}},{"cell_type":"markdown","source":"## Age->Continous Feature","metadata":{}},{"cell_type":"code","source":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')\n# 연속값인 나이를 정리하기 전 최대, 최소 값과 평균값을 확인한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.001868Z","iopub.execute_input":"2022-07-22T01:30:42.003053Z","iopub.status.idle":"2022-07-22T01:30:42.013048Z","shell.execute_reply.started":"2022-07-22T01:30:42.003000Z","shell.execute_reply":"2022-07-22T01:30:42.011805Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\",hue=\"Survived\",data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\",hue=\"Survived\", data=data,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()\n\n# 바이올린 플롯을 이용해 클래스와 나이 를 생존/비생존으로 나눠서 확인\n# \"                  성별과 나이를       \"           나눠서 확인\n# 솔찍히 바이올린 플롯은 보기 익숙하지 않아서 한번에 눈에 띄지 않는데 왜 쓰는지 모르겠다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.016203Z","iopub.execute_input":"2022-07-22T01:30:42.016981Z","iopub.status.idle":"2022-07-22T01:30:42.600281Z","shell.execute_reply.started":"2022-07-22T01:30:42.016934Z","shell.execute_reply":"2022-07-22T01:30:42.599006Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.   \n어린이의 수는 pclass와 함께 증가하며 10세 이하의 생존율은 pclass와 관계업이 양호하다\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.   \n1클래스에 20-25세의 승객들의 생존 기회는 높다 심지어 여성에게 더 높다\n\n3)For males, the survival chances decreases with an increase in age.   \n남성에게 생존기회는 나이와 함께 감소된다","metadata":{}},{"cell_type":"markdown","source":"As we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.   \n우리가 이전에 봤기때문에, 나이특징은 177개의 널값을 가진다 이러한 널값을 교체하기 위해 우리는 그것들이 데이터셋에 mean age가 할당할수 있다.\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie??   \n그러나 문제는 많은 다른나이를 가진 사람이 있다. 우리는 4세 아이에게 29세 성인과 함께 할당할수 없다. 승객이 거짓말을 하는 연령대를 알 수 있는 방법이 있나\n\nBingo!!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups.    \n맞다. 우리는 이름 기능을 확인 할 수있다. 위의 특징을 보면, 우리는 이름에 mr.mrs와 같은 세츄레이션을 가지는걸 볼 수 있다. 그러므로 우리는 mr,mrs에 평균 값을 각 그룹에 할당할수 있다","metadata":{}},{"cell_type":"markdown","source":"### ''What's In A Name??''---> Feature :p","metadata":{}},{"cell_type":"code","source":"data['Initial']=0\n# 데이터 셋에 새로운 칼럼을 만들겠다\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')   #lets extract the Salutations\n \n\n#\n# initial 값을 초기화 시킨후 데이터의 이름목록에서 .이붙은 우리가 원하는 이름만 모아서 넣는다\n# 영어 이름엔 Mr. Mrs. 와 같은 타이틀이 붙는데 타이틀 끝에는 .이 붙기 때문에 위의 정규 표현식에\n# 대소문자 관계없이 + . 으로 문자열을 탐색을 해서 initial 칼럼에 넣어 준다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.602145Z","iopub.execute_input":"2022-07-22T01:30:42.602886Z","iopub.status.idle":"2022-07-22T01:30:42.667579Z","shell.execute_reply.started":"2022-07-22T01:30:42.602828Z","shell.execute_reply":"2022-07-22T01:30:42.666441Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Okay so here we are using the Regex: [A-Za-z]+).. So what it does is, it looks for strings which lie between A-Z or a-z and followed by a .(dot). So we successfully extract the Initials from the Name.     \n여기서 우리는 Regex를 사용한다 그래서 이것은 문장을 찾는다 ","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex\n# 위에서 만든 이니셜 칼럼을 성별로 나눠서 확인한다.","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.669121Z","iopub.execute_input":"2022-07-22T01:30:42.669877Z","iopub.status.idle":"2022-07-22T01:30:42.721824Z","shell.execute_reply.started":"2022-07-22T01:30:42.669830Z","shell.execute_reply":"2022-07-22T01:30:42.720831Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n# 타이틀이 너무 많기 때문에 남성 타이틀은 mr, 여성 타이틀은 mrs, 나머지는 other로 치환해준다 ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.723873Z","iopub.execute_input":"2022-07-22T01:30:42.724571Z","iopub.status.idle":"2022-07-22T01:30:42.735625Z","shell.execute_reply.started":"2022-07-22T01:30:42.724514Z","shell.execute_reply":"2022-07-22T01:30:42.733939Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean() #lets check the average age by Initia\n\n# 치환한 initial 칼럼을 나이별로 묶고 평균을 내어서 타이틀이 가진 나이대를 확인한다\n# master는 어린이, mr는 중년남성, mrs는 중년여성, 나머지는 그 이상의 나이대임을 확인 할 수 있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.737780Z","iopub.execute_input":"2022-07-22T01:30:42.738512Z","iopub.status.idle":"2022-07-22T01:30:42.751155Z","shell.execute_reply.started":"2022-07-22T01:30:42.738476Z","shell.execute_reply":"2022-07-22T01:30:42.748696Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Assigning the NaN values with the Cell values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\n# 위의 타이틀별 나이대의 평균을 확인 하였으니 널값을 채울수 있다\n#loc 치환해준다 age가 널값이며 Initial이 Mr인 사람을 33값으로 치환해준다\n#널 값들을 합리적으로 값을 정해준다\n# 널값을 채워주는 핵심 포인트 ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.752496Z","iopub.execute_input":"2022-07-22T01:30:42.753725Z","iopub.status.idle":"2022-07-22T01:30:42.771258Z","shell.execute_reply.started":"2022-07-22T01:30:42.753675Z","shell.execute_reply":"2022-07-22T01:30:42.769363Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any() # So no null valued left finally\n# 남은 null 값이 있는지 한번 더 체크한다\n# False가 나왔으니 합리적으로 나이대의 널값을 채웠음을 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.772826Z","iopub.execute_input":"2022-07-22T01:30:42.773537Z","iopub.status.idle":"2022-07-22T01:30:42.783225Z","shell.execute_reply.started":"2022-07-22T01:30:42.773492Z","shell.execute_reply":"2022-07-22T01:30:42.781970Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize = (20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived = 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived=1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()\n\n# 왼쪽엔 사망자 오른쪽엔 생존자로 구간을 나누고 나이대 별로 확인 할 수 있게 히스토그램을 활용한 시각\n# hue를쓰면 색갈로 구분 col을 쓰면 칼럼으로 구분","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:42.786037Z","iopub.execute_input":"2022-07-22T01:30:42.786770Z","iopub.status.idle":"2022-07-22T01:30:43.348102Z","shell.execute_reply.started":"2022-07-22T01:30:42.786705Z","shell.execute_reply":"2022-07-22T01:30:43.346922Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"- Observations:\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n5세 이하에 많은 사람들이 살아남았다\n2)The oldest Passenger was saved(80 years).\n80세 이상은 살아남았다\n3)Maximum number of deaths were in the age group of 30-40.\n30-40대에서 많은수가 사망하였\n","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass','Survived',col='Initial',data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:43.349610Z","iopub.execute_input":"2022-07-22T01:30:43.349973Z","iopub.status.idle":"2022-07-22T01:30:44.707486Z","shell.execute_reply.started":"2022-07-22T01:30:43.349940Z","shell.execute_reply":"2022-07-22T01:30:44.706282Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"- The Women and Child first policy thus holds true irrespective of the class.\n여성과 아이는 클래스에 관계없이 우선 살린","metadata":{}},{"cell_type":"markdown","source":"## Embarked -> Categorical Value","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')\n# 항구와 pclass를 인덱스 성별과 survived를 칼럼으로 하여 표를 만들고 연관성을 확인한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:44.709130Z","iopub.execute_input":"2022-07-22T01:30:44.710128Z","iopub.status.idle":"2022-07-22T01:30:44.793425Z","shell.execute_reply.started":"2022-07-22T01:30:44.710078Z","shell.execute_reply":"2022-07-22T01:30:44.792000Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"- Chances for Survival by Port Of Embarkation\n 승선항의 생존 가능","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked','Survived',data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()\n\n# 승선항과 생존의 관계를 확인하기위한 시각화","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:44.794736Z","iopub.execute_input":"2022-07-22T01:30:44.795100Z","iopub.status.idle":"2022-07-22T01:30:45.149291Z","shell.execute_reply.started":"2022-07-22T01:30:44.795070Z","shell.execute_reply":"2022-07-22T01:30:45.148005Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"- The chances for survival for Port C is highest around 0.55 while it is lowest for S.","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()\n\n# 승선항과 성별/ 승선항과 생존 / 승선항과 pclass별로 나눠서 countplot으로 나눠서 확인한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:45.151072Z","iopub.execute_input":"2022-07-22T01:30:45.151442Z","iopub.status.idle":"2022-07-22T01:30:45.792193Z","shell.execute_reply.started":"2022-07-22T01:30:45.151409Z","shell.execute_reply":"2022-07-22T01:30:45.790760Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.      \ns에서 가장 많은 승객이 탑승했다. 그들의 대다수는 pclass3 이다\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.       \nc의 승객들은 생존비율이 높다. 그 이유는 pclass1 과 pclass2승객의 비율이 높기 때문이다\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.   \ns항구는 대다수의 부자들이 탑승한 항구를 바라보고 있다. 여전히 생존율은 낮다 pclass3의 승객중 81%가 생존하지 못했기때문이다 \n\n4)Port Q had almost 95% of the passengers were from Pclass3.     \nQ항구는 pclass3의 구성원이 거의 95%이다 ","metadata":{}},{"cell_type":"code","source":"sns. factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\nplt.show()\n# pclass와 survived로 표를 만들고 항구별로 분류한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:45.793818Z","iopub.execute_input":"2022-07-22T01:30:45.794172Z","iopub.status.idle":"2022-07-22T01:30:47.058237Z","shell.execute_reply.started":"2022-07-22T01:30:45.794140Z","shell.execute_reply":"2022-07-22T01:30:47.057055Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n\n생존기회는 pclass와 관계없이 1순위다\n\n2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.(Money Matters)\n\ns항구는 매우 불행해 보인다 pclass3승객들에게 남성과 여성의 생존비율이 낮기 때문에(돈이유)\n\n3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\n\nq항구는 남성에게 가장안좋다\n\n### Filling Embarked NaN\nAs we saw that maximum passengers boarded from Port S, we replace NaN with S.\n\n우리는 s로부터 최대 승객이 탑승했다는것을 봤다. 우리는 nan을 nan을 s로 바꿀것이","metadata":{}},{"cell_type":"code","source":"data['Embarked'].fillna('S',inplace=True)\n\n# 항구의 널데이터 값이 2개 뿐이라 s로 채워준다(s항구에서 가장 많은 사람이 탑승했기때문)\n# 이제 cabin의 널값만 남았다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.059889Z","iopub.execute_input":"2022-07-22T01:30:47.060255Z","iopub.status.idle":"2022-07-22T01:30:47.067048Z","shell.execute_reply.started":"2022-07-22T01:30:47.060219Z","shell.execute_reply":"2022-07-22T01:30:47.065830Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### fillna()\n\n결측값을 ()로 채운다","metadata":{}},{"cell_type":"code","source":"data.Embarked.isnull().any()  # Finally No NaN values\n\n# 항구에 널값이 있는지 확인","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.068513Z","iopub.execute_input":"2022-07-22T01:30:47.068910Z","iopub.status.idle":"2022-07-22T01:30:47.082509Z","shell.execute_reply.started":"2022-07-22T01:30:47.068875Z","shell.execute_reply":"2022-07-22T01:30:47.081728Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### SibSip-->Discrete Feature\nThis feature represents whether a person is alone or with his family members.\n\n이 특징은 혼자인지 그의 가족과 함께인지를 대표한다\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')\n# 자녀가 많을수록 생존율이 떨어지며 3명부터 급격하게 줄어드는 것을 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.090325Z","iopub.execute_input":"2022-07-22T01:30:47.090917Z","iopub.status.idle":"2022-07-22T01:30:47.120944Z","shell.execute_reply.started":"2022-07-22T01:30:47.090880Z","shell.execute_reply":"2022-07-22T01:30:47.119623Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=data,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.pointplot('SibSp','Survived',data=data,ax=ax[1])\nax[1].set_title('SibSp vs Survived')#- 왜 이걸 쓰면 안보이는가???\n\nplt.close(2)\n\nplt.show()\n\n# 자녀수와 생존율을 bar와 point로 각각 시각화 하여 살펴 보자 (insight를 얻기 위해)\n# 확실히 한명있을때의 생존율이 가장 높으며 많아질수록 생존율이 떨어지는 것을 쉽게 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.122328Z","iopub.execute_input":"2022-07-22T01:30:47.122636Z","iopub.status.idle":"2022-07-22T01:30:47.944577Z","shell.execute_reply.started":"2022-07-22T01:30:47.122607Z","shell.execute_reply":"2022-07-22T01:30:47.943039Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap = 'summer_r')\n# 이번엔 자녀수와 pclass별로 확인 해 본다\n# 1,2 클래스와 달리 3클래스에는 대가족들이 많은 것을 확인 할 수 있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.946471Z","iopub.execute_input":"2022-07-22T01:30:47.946897Z","iopub.status.idle":"2022-07-22T01:30:47.982006Z","shell.execute_reply.started":"2022-07-22T01:30:47.946855Z","shell.execute_reply":"2022-07-22T01:30:47.980982Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"- Observations:\nThe barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is 0%. The reason may be Pclass??       \n바플롯과 팩터플롯은 만약 승객이 형제없이 혼자 탑승하는 경우 생존율이 34.5%임을 보여준다. 형제수가 증가하면 그래프는 대략 감소한다  이것은 의미가 있습니다 만약 내가족이 탑승했다면 나를 구하는것보다 가족을 먼저 구할것이다 놀랍게도 5-8구성원의 가족들의 생존율은 0%이다. pclass가 이유인가?\n\nThe reason is Pclass. The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.      \npclass가 이유다 크로스탭은 3명이상의 구성원을 가진 사람들이 모두 pclass3이다  생존하지 못한 pclass3의 3명이상의 대가족들은 모두 임박했다?  ","metadata":{}},{"cell_type":"markdown","source":"### Parch","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')\n\n# 부모자녀와 pclass로 비교해 본다\n# 여기서 또한 3class에 대가족이 많은것을 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:47.983283Z","iopub.execute_input":"2022-07-22T01:30:47.984087Z","iopub.status.idle":"2022-07-22T01:30:48.021531Z","shell.execute_reply.started":"2022-07-22T01:30:47.984049Z","shell.execute_reply":"2022-07-22T01:30:48.020688Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The crosstab again shows that larger families were in Pclass3.\n크로스탭은 다시 보여준다 대가족이 pclass3에 있다는걸","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.pointplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')  #- 왜 2열에 선언하지 않은 표가 나오는가??? \nplt.close(2)\nplt.show()\n\n# 이번에도 bar, pointplot을 이용해 시각화 해본다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:48.022791Z","iopub.execute_input":"2022-07-22T01:30:48.023282Z","iopub.status.idle":"2022-07-22T01:30:48.772196Z","shell.execute_reply.started":"2022-07-22T01:30:48.023252Z","shell.execute_reply":"2022-07-22T01:30:48.771047Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"- Observations:\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.       \n여기에 꽤 비슷한 결과가 있다.그들의 부모와 함께 탑승한 승객들은 생존에 더큰 기회가 있다. 그러나 숫자가 올라갈수록 감소합니다 \nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.     \n생존의 기회는 1-3명의 부모를 가진 자들에게 더 높다\n혼자탄 사람은 치명적이며 4명이상의 부모를 가진경우 생존 가능성이 감소합니다\n\n# 즉, 가족구성원이 많을수록 생존율은 급격히 줄어들고 3class에 대가족들이 많아서 생존율이 낮다","metadata":{}},{"cell_type":"markdown","source":"### Fare -> Continous Feature","metadata":{}},{"cell_type":"code","source":"print('Highest Fare was:', data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())\n\n# 요금의 최대값 최소값 평균을 구한다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:48.773261Z","iopub.execute_input":"2022-07-22T01:30:48.773564Z","iopub.status.idle":"2022-07-22T01:30:48.783795Z","shell.execute_reply.started":"2022-07-22T01:30:48.773536Z","shell.execute_reply":"2022-07-22T01:30:48.782423Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"- The lowest fare is 0.0. Wow!! a free luxorious ride.\n/ 가장 낮은 요금은 0원이다!! 공짜 럭셔리 탑승","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(1,3,figsize = (20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()\n\n# 클래스 별로 요금에 따라 시각화 분류\n# 1클래스의 가격이 나머지 클래스와 5배 정도 차이나는걸 볼수 있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:48.785162Z","iopub.execute_input":"2022-07-22T01:30:48.786030Z","iopub.status.idle":"2022-07-22T01:30:49.452696Z","shell.execute_reply.started":"2022-07-22T01:30:48.785994Z","shell.execute_reply":"2022-07-22T01:30:49.451413Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning.\n\npclass1에서 승객의 요금에 큰 차이가 있다 이 분포는 기준이 감소함에 따라 계속 감소할것이다\n이것은 연속적이기 때문에 우리는 빈닝을 사용함으로써 이산값으로 변환할 수 있습니다. ","metadata":{}},{"cell_type":"markdown","source":"### Observations in a Nutshell for all features:\nSex: The chance of survival for women is high as compared to men.\n\n성별: 여성의 생존의 기회가 남성과 비교해 높다\n\nPclass:There is a visible trend that being a 1st class passenger gives you better chances of survival. The survival rate for Pclass3 is very low. For women, the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. Money Wins!!!.\n\npclass: 1클래스 승객들이 더나은 생존기회를 주는 경향이 보인다. 3클래스의 생존비율이 매우 낮다. 여성들에겐 1클래스의 생존기회가 2클래스 보다 더 높다 돈의승리! \n\nAge: Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n\n나이: 5-10세의 아이들은 생존에 높은 기회가 있다. 15-35세의 승객들은 많이 죽었다\n\nEmbarked: This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass3.\n\n항구: 이것은 매우 흥미로운 특징이다. c항구에서의 생존기회가 더 높다 비록 pclass1의 승객들이 주로 s에 있긴하지만.  Q의 승객들은 모두 pclass3이\n\nParch+SibSp: Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.\n\nparch + SibSp: 1-2개의 형재, 배우자, 1-3명의 부모님을 가진경우는 혼자이거나  대가족을 가진팀보다 더 나은 개연을 보여준다","metadata":{}},{"cell_type":"markdown","source":"### Correlation Between The Features","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)  #data.corr()-->correlation matrix\n# annot은 숫자를 보여주는것\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:49.454130Z","iopub.execute_input":"2022-07-22T01:30:49.454469Z","iopub.status.idle":"2022-07-22T01:30:49.948280Z","shell.execute_reply.started":"2022-07-22T01:30:49.454440Z","shell.execute_reply":"2022-07-22T01:30:49.947079Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"얼마나 상관관계가 있는가 1로 갈수록 상관 up","metadata":{}},{"cell_type":"markdown","source":"- Interpreting The Heatmap (히트맵 설명?통역?)\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n노트에서 첫번째 할것은 알파벳이나 문자열 사이의 상관관계를 알 수 없다는 것이다. 플롯을 이해하기 전에는 상관관계가 정확히 무엇인지 살펴보자!\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\n긍정적인 상관관계: 만약 A가 증가가 B의 증가로 이어진다면 그들은 긍정적인 상관관계라 한다 value1은 긍정적 상관관계를 의미한다 \n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\n부정적인 상관관계 : 만약 a가 증가하는데 b가 감소할때 그들은 부정적인 상관관계이다 값 -1은 완전한 음의 상관관계를 의미한다\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\n이제 두 기능이 고도로 또는 완벽하게 상관관계가 있으므로 하나의 증가가 다른 특성의 증가로 이어진다고 가정해 보겠습니다. 이는 두 기능 모두 매우 유사한 정보를 포함하고 있으며 정보의 변동이 거의 또는 전혀 없음을 의미합니다. 둘 다 거의 동일한 정보를 포함하므로 이름 다중공선성이라고 합니다\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\n그래서 우리는 그들 둘다를 사용하도록 생각해보자 그들중 하나는 불필요하다. 모델을 만들거나 훈련시키는동안 우리는 불필요한 특징들을 제거해야한다 이것이 훈련시간과 많은 이점들을 줄이기 때문에\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features.\n\n지금부터 위의 히트맵에서 우리는 특징들이 상관관계가 많진않다는것을 볼 수 있다. 가장높은 상관관계는 형제와 자매관계 즉 0.41이다 그래서 우리는 모든 기능을 계속 사용할 수 있다","metadata":{}},{"cell_type":"markdown","source":"## Part2: Feature Engineering and Data Cleaning(데이터 정리와 피쳐 엔지니어링!)\n\nNow what is Feature Engineering?\n\n이제 피쳐 엔지니어링은 무엇인가?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n\n우리가 데이터셋을 받을때마다 모든 피쳐가 중요하진 않다. 아마 많은 불필요한 피쳐들을 제거해야할 필요가 있다. 또한 우리는 다른 피쳐들의 정보들을 관찰하고 추적함으로써 새로운 피쳐들을 얻을수 있다  \n\nAn example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.\n\n예로 이름특징을 사용하여 이니셜 피쳐를 가져오는것이 있다. 새로운 기능을 얻고 몇가지를 제거 할 수 있는지 보자. 또한 기존 관련 기능을 predictive Modeling에 적합한 형식으로 변환합니다","metadata":{}},{"cell_type":"markdown","source":"### Age_band\nProblem With Age Feature:\nAs I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.\n\n나이특성과 문제:\n내가 앞에서 나이는 연속적인 특징이라고 언급했었다. 머신러닝 모델에서 연속적인 변수는 문제가 있다\n\nEg:If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.\n\n예) 만약 내가 스포츠인을 그룹화 하거나 말하거나 정렬한다면 우리는 쉽게 그들을 남성과 여성으로 분류할 수 있다. \n\nNow if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\n\n지금부터 만약 내가 그들을 나이로 그룹화 하라고 한다면 어떻게 할래? 만약 30명이 있고 그들은 30의 나이 값이 있다 이것은 문제가 된다\n\nWe need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n\n우리는 binning 또는 normalization을 통해 이러한 연속값들을 범주형 값으로 바꿔줘야 할 필요가 있다 초보와 일반으로 나는 바인잉을 사용할 것이다 즉, 연령범위를 단일빈으로 그룹화 하거나 단일값을 할당 합니다.\n\nOkay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16. So bins of size 16.\n\n이제 승객의 최고령은 80 이었다 함께 나눠보자 0-80의 범위를 5가지 bin으로 80/5는 16이니까 bin 사이즈는 16이","metadata":{}},{"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(2)\n\n# 연속값인 Age를 5개의 범주로 만든다\n# 기준은 최대 나이가 80이니 80/5 인 16씩 기준으로 나눈다  ==>  데이터 binning기","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:49.950039Z","iopub.execute_input":"2022-07-22T01:30:49.950809Z","iopub.status.idle":"2022-07-22T01:30:49.981622Z","shell.execute_reply.started":"2022-07-22T01:30:49.950730Z","shell.execute_reply":"2022-07-22T01:30:49.980525Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')\n# 위에서 만든 age_band를 갯수를 세서 시각화 한다\n# 17~32세, 33~48세가 대부분이 ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:49.983176Z","iopub.execute_input":"2022-07-22T01:30:49.983531Z","iopub.status.idle":"2022-07-22T01:30:50.000674Z","shell.execute_reply.started":"2022-07-22T01:30:49.983500Z","shell.execute_reply":"2022-07-22T01:30:49.999712Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Age_band','Survived',data=data,col='Pclass')\nplt.show()\n\n# 나이대와 생존율을 pclass별로 나눠서 시각화\n# 나이가 어릴수록 생존율이 높다, 클래스가 높을수록 생존율이 높다, ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:50.002218Z","iopub.execute_input":"2022-07-22T01:30:50.002894Z","iopub.status.idle":"2022-07-22T01:30:51.193434Z","shell.execute_reply.started":"2022-07-22T01:30:50.002857Z","shell.execute_reply":"2022-07-22T01:30:51.192169Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"True that..the survival rate decreases as the age increases irrespective of the Pclass.\n\n사실 생존율은 클레스에 관계없이 나이가 증가함에 따라 감소한다","metadata":{}},{"cell_type":"markdown","source":"## Family_Size and Alone\nAt this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.\n\n여기서 우리는 familly_size 와 alone 이라는 새로운 피쳐를 만들고 분석할수 있다. 이 특징은 부모와 형제의 총합이다. 생존율이 탑승자의 가족규모와 관련이 있는지 확인할 수 있도록 결합된 데이터를 제공한다. 홀로 승객이 혼자인지 아닌지를 나타낼것이다.","metadata":{}},{"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n# 데이터에 부모자녀, 형제자매 칼럼 두개를 합쳐서 가족사이즈라는 칼럼을 새로 만들고, alone 칼럼또한 만든다\n# 가족사이즈 칼럼에서 혼자인 사람들을 alone칼럼에 채워준다\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.pointplot('Family_Size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.pointplot('Alone','Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()\n# 가족 사이즈 별 생존율과 혼자일때의 생존율을 함께 본다\n# 사족수는 3명을 기준으로 4명부터는 급격한 생존율의 감소를 보인다\n# 혼자탈때는 생존율이 낮은것을 확인 할 수있다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:51.195082Z","iopub.execute_input":"2022-07-22T01:30:51.195447Z","iopub.status.idle":"2022-07-22T01:30:51.893847Z","shell.execute_reply.started":"2022-07-22T01:30:51.195415Z","shell.execute_reply":"2022-07-22T01:30:51.892559Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"- Family_Size=0 means that the passeneger is alone. Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further.\n\n가족사이즈 = 0 은 승객이 혼자라는것을 의미한다 명확하게, 만약 니가 혼자라면 생존의 기회는 매우 낮다 가족크기가 4명 초과라면 생존율은 더 감소한다. 이것은 모델에 중요한 기능을 한다. 함께 이미래를 검사해보자","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')\nplt.show()\n\n# 혼자탄사람을 성별, pclass별로 나눠서 확인 해본다\n# 여성들은 pclass에 상관없이 생존율이 높으며, 특히 1,2 클래스에 혼자탄 여성은 대부분 생존했다\n# 하지만 3클래스에 혼자 탑승한 사람들은 모두 생존율이 낮다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:51.895741Z","iopub.execute_input":"2022-07-22T01:30:51.896134Z","iopub.status.idle":"2022-07-22T01:30:53.071774Z","shell.execute_reply.started":"2022-07-22T01:30:51.896101Z","shell.execute_reply":"2022-07-22T01:30:53.070410Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"- It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n\n이것은 혼자타는것이 위험하다는걸 보여준다 pclass3를 제외하고 성별과 plcass에 관계없이 혼자인 여선의 기회는 가족과 함께인것보다 높다 이것은 성별과 pclass에 관계없이 혼자타는 것은 해롭다는 걸 보여준다 plcass3에서는 예외적으로 가족보다 혼자타는게 더 높다","metadata":{}},{"cell_type":"markdown","source":"## Fare_Range\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.\n\n요금 또한 연속적인 특징이기 때문에 우리는 이것을 정규변수로써 변환할 필요가 있다. \n여기에서 우리는 판다스.qcut을 사용할 것이다\n\nSo what qcut does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into \n5 seperate bins or value ranges.\n\nqcut은 우리가 통과한 bins의 수에 따라 값을 나누거나 배열하는 것이다. 만약 우리가 5개의 bin을 넘겼다면, 이것은 값들을 같은 간격으로 배열할 것이다. 동등한 공간에 5개의 개별 빈 또는 범위값","metadata":{}},{"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')\n\n# 요금또한 연속적인 값이기 때문에 4등분 해준다\n# 4등분 한것을 생존율과 묶어서 평균화 해준다 \n# 돈 많은 사람을 먼저 살리지 않았을까 하는 생각에 이렇게 묶은것 같다\n# pd.qcut : 판다스 함수로써 나누고 싶은 개수를 정해주면 알아서 나눠주는 함수","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:53.073227Z","iopub.execute_input":"2022-07-22T01:30:53.074088Z","iopub.status.idle":"2022-07-22T01:30:53.105309Z","shell.execute_reply.started":"2022-07-22T01:30:53.074050Z","shell.execute_reply":"2022-07-22T01:30:53.104100Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"- As discussed above, we can clearly see that as the fare_range increases, the chances of survival increases.\n\n위에서 논의한 것과같이, 우리는 요금범위가 증가함에 따라 생존가능성이 증가한다는걸 명확하게 볼 수 있다\n\n- Now we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in Age_Band\n\n이제 우리는 요금범위를 넘길수 없다. 우리는 나이밴드와 같이 값을 바꿔야한","metadata":{}},{"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<31),'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3\n\n# 4개로 나눈 요금 구간을 0,1,2,3의 값으로 치환해서 새로만든 칼럼에 넣어준다\n# 이렇게 값을 바꿔주는 이유는 시각화 했을때 보기 쉽게 하기 위해서 인것 같다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:53.107113Z","iopub.execute_input":"2022-07-22T01:30:53.107442Z","iopub.status.idle":"2022-07-22T01:30:53.124977Z","shell.execute_reply.started":"2022-07-22T01:30:53.107412Z","shell.execute_reply":"2022-07-22T01:30:53.122871Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()\n\n# 위에서 만든 요금 칼럼을 성별을 기준으로 시각화 하였다\n#  ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:53.126647Z","iopub.execute_input":"2022-07-22T01:30:53.127146Z","iopub.status.idle":"2022-07-22T01:30:53.770715Z","shell.execute_reply.started":"2022-07-22T01:30:53.127106Z","shell.execute_reply":"2022-07-22T01:30:53.769835Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Clearly, as the Fare_cat increases, the survival chances increases. This feature may become an important feature during modeling along with the Sex.\n\n명확하게 요금 캣이 증가함에 따라, 생존 기회는 증가한다. 이 특징은 아마 중요한 기능이 될 것이다. 성별과 함께 모델링 하는 동안","metadata":{}},{"cell_type":"markdown","source":"### Converting String Values into Numeric\nSince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.\n\n우리가 머신러닝 모델에 문장을 넣을수 없기 때문에 우리는 성별, 항구 등등 다양한 값을 바꿔줘야한다","metadata":{}},{"cell_type":"code","source":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\n\n# 머신러닝 모델에 문장을 넣을 수 없기 때문에 문장을 값으로 바꿔준다\n# 남자는 0으로 여자는1로 바꾼다\n# 항구또한 값으로 바꿔준다\n# 타이틀도 값으로 바꿔준다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:53.772423Z","iopub.execute_input":"2022-07-22T01:30:53.773140Z","iopub.status.idle":"2022-07-22T01:30:53.787385Z","shell.execute_reply.started":"2022-07-22T01:30:53.773095Z","shell.execute_reply":"2022-07-22T01:30:53.786420Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"- Dropping UnNeeded Features\nName--> We don't need name feature as it cannot be converted into any categorical value.\n\n이름 --> 우리는 어떤 범주형값으로 바꿀수 없는 이름 기능은 필요없다\n\nAge--> We have the Age_band feature, so no need of this.\n\n나이 --> 우리는 나이밴드 특징을 가지고 있다, 그래서 필요없다\n\nTicket--> It is any random string that cannot be categorised.\n\n티켓--> 이것은 카테고리화될수 없는 랜덤 문장이다\n\nFare--> We have the Fare_cat feature, so unneeded\n\n요금 --> 우리는 요금 카테고리 특징이 있다, 그래서 필요없다\n\nCabin--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\n   -- > 많은 널값과 많은 승객들은 멀티플한 캐빈을 가진다. 그래서 이것은 필요없는 기능이다\nFare_Range--> We have the fare_cat feature.\n\nPassengerId--> Cannot be categorised.\n\n승객id --> 카테고리화 할수 없다","metadata":{}},{"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()\n\n# 필요없는 것들 드랍시켜서 없애버림 ==> 머신러닝에 넣을 데이터를 정리한다\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:53.789034Z","iopub.execute_input":"2022-07-22T01:30:53.789776Z","iopub.status.idle":"2022-07-22T01:30:54.635638Z","shell.execute_reply.started":"2022-07-22T01:30:53.789712Z","shell.execute_reply":"2022-07-22T01:30:54.634463Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Now the above correlation plot, we can see some positively related features. Some of them being SibSp andd Family_Size and Parch and Family_Size and some negative ones like Alone and Family_Size.\n\n이제 위의 상관 관계 플롯에서 몇 가지 긍정적인 관련 기능을 볼 수 있습니다. 그들 중 일부는 SibSp 및 Family_Size 및 Parch 및 Family_Size이고 일부는 Alone 및 Family_Size와 같은 부정적인 것입니다.","metadata":{}},{"cell_type":"markdown","source":"\n#### 알고리즘의 공부 이유 위의 파라미터들이 어떻게 쓰이는지 알기 위해 \n#### 파라미터가 왜중요한가? 파라미터를 어떻게 지정해주느냐에 따라 모델의 성능이 달라지기 때문에\n#### 그러기 위해선 각각의 알고리즘이 가지는 특성과 파라미터에 대한 이해와 감 경험이 필요\n#### 하이퍼 파라미터 조정을 어떻게 하는가  ==> 메뉴얼대로 해서 감을 쌓아라\n#### 파라미터 튜닝은 중요한 요소이다","metadata":{}},{"cell_type":"markdown","source":"## Part3: Predictive Modeling\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n\n우리는 몇몇 인사이트를 얻었따 eda 부분으로부터 . 그러나 우리는 정확하게 예측하거나 승객들이 살지 죽을지 말할수 없다. 그래서 우리는 승객이 살지 안살지 많은 훌륭한 분류 알고리즘을 사용해서 예측할것이다\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","metadata":{}},{"cell_type":"code","source":"# importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression # logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split # training and testing data split\nfrom sklearn import metrics # accuracy measure\nfrom sklearn.metrics import confusion_matrix # for confusion matrix\n\n#사용할 알고리즘들 호출","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:54.637422Z","iopub.execute_input":"2022-07-22T01:30:54.638583Z","iopub.status.idle":"2022-07-22T01:30:55.010411Z","shell.execute_reply.started":"2022-07-22T01:30:54.638534Z","shell.execute_reply":"2022-07-22T01:30:55.009169Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train,test = train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']\n\n# 훈련용과 테스트용 나눔 테스트용(30%)\n# 칼럼값들을 가져오겠다\n# stratify는 - 타겟 변수로 지정해주면 비율에 맞게 나눠준다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.011625Z","iopub.execute_input":"2022-07-22T01:30:55.011974Z","iopub.status.idle":"2022-07-22T01:30:55.027277Z","shell.execute_reply.started":"2022-07-22T01:30:55.011943Z","shell.execute_reply":"2022-07-22T01:30:55.025877Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## Radial Support Vector Machines(rbf-SVM)","metadata":{}},{"cell_type":"markdown","source":"- 선형 svm에선 사용할 수 없는 상황에서 사용하는 커널이다\n- 2차원 상에서 선형으로 분류할 수 없던 데이터들이 고차원으로 확장하자 가능해졌다.\n\n- 커널의 종류: Polynomal / Sigmoid / 가우시안 rbf\n- 가우시안 rbf의 성능이 뛰어나다 \n- 가우시안 rbf의 하이퍼 파라미터 : gamma, C\ngamma: gamma가 클수록 데이터 포인터들이 영향력을 행사하는 거리가 짧아진다, gamma가 커지면 작은 표준편차를 갖는다\nC: C가 커질수록 이상치의 존재를 무사하기 힘들어진다\n이 두 파라미터가 낮으면 underfitting, 높으면 overfitting의 위험이 있다.","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))\n\n# 정확도 체크","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.028936Z","iopub.execute_input":"2022-07-22T01:30:55.029788Z","iopub.status.idle":"2022-07-22T01:30:55.063936Z","shell.execute_reply.started":"2022-07-22T01:30:55.029716Z","shell.execute_reply":"2022-07-22T01:30:55.062797Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"- 피쳐들을 만들때 관리하기위한 변수?에 넣어주고 관리를 해줘야한","metadata":{}},{"cell_type":"markdown","source":"### Linear Support Vector Machine(linear-SVM)","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))\n# 위와 같은 알고리즘이지만 파라미터를 조정(커널을 바꿈)\n# 왜 바꿨으며 바꿔서 뭐가 달라졌는가\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.065101Z","iopub.execute_input":"2022-07-22T01:30:55.066009Z","iopub.status.idle":"2022-07-22T01:30:55.092311Z","shell.execute_reply.started":"2022-07-22T01:30:55.065968Z","shell.execute_reply":"2022-07-22T01:30:55.091003Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.093805Z","iopub.execute_input":"2022-07-22T01:30:55.094249Z","iopub.status.idle":"2022-07-22T01:30:55.121409Z","shell.execute_reply.started":"2022-07-22T01:30:55.094214Z","shell.execute_reply":"2022-07-22T01:30:55.120104Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.122675Z","iopub.execute_input":"2022-07-22T01:30:55.123020Z","iopub.status.idle":"2022-07-22T01:30:55.137457Z","shell.execute_reply.started":"2022-07-22T01:30:55.122990Z","shell.execute_reply":"2022-07-22T01:30:55.136649Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neighbours(KNN)","metadata":{}},{"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.138313Z","iopub.execute_input":"2022-07-22T01:30:55.138642Z","iopub.status.idle":"2022-07-22T01:30:55.165183Z","shell.execute_reply.started":"2022-07-22T01:30:55.138615Z","shell.execute_reply":"2022-07-22T01:30:55.164123Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"- Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default value is 5. Lets check the accuracies over various values of n_neighbours.\n\n이제 n_neighbours에 속성값을 변경하면 knn모델에 대한 정확도가 바뀐다  \nn_neighbours의 다양한 에 대한 정확도를 확인해보","metadata":{}},{"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.166607Z","iopub.execute_input":"2022-07-22T01:30:55.167608Z","iopub.status.idle":"2022-07-22T01:30:55.549309Z","shell.execute_reply.started":"2022-07-22T01:30:55.167573Z","shell.execute_reply":"2022-07-22T01:30:55.547781Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.550810Z","iopub.execute_input":"2022-07-22T01:30:55.551273Z","iopub.status.idle":"2022-07-22T01:30:55.565211Z","shell.execute_reply.started":"2022-07-22T01:30:55.551239Z","shell.execute_reply":"2022-07-22T01:30:55.564217Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.566604Z","iopub.execute_input":"2022-07-22T01:30:55.567767Z","iopub.status.idle":"2022-07-22T01:30:55.798518Z","shell.execute_reply.started":"2022-07-22T01:30:55.567705Z","shell.execute_reply":"2022-07-22T01:30:55.797145Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\n모델의 정확도가 분류의 견고함들을 결정하는 요소가 아니다  분류기가 훈련 데이터에 대해 훈련되고 테스트 데이터에 대해 테스트되었으며 90%의 정확도를 기록했다고 가정해 보겠습니다. \n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance.\n\n이제 이것은 좋은 정확도를 가진 분류가 된거같다. 회귀에 대해 그러나 우리는 모든 새로운 테스트 셋에 대해 90라는 것을 확인 할 것인가?\n그 대답은 아니다, 우리는 결정할수 없다. 분류기가 자체 학습에 사용할 모든 인스턴스를 결정할 수는 없습니다. 훈련 및 테스트 데이터가 변경되면 정확도도 변경됩니다. 증가하거나 감소할 수 있습니다. 이것을 모델 분산이라고 합니다.\n\nTo overcome this and get a generalized model,we use Cross Validation.\n\n이것을 극복하기위해 그리고 일반화된 모델을 얻기위해 우리는 교차 검증을 사용해야 합니다","metadata":{}},{"cell_type":"markdown","source":"## Cross Validation\n\nMany a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n\n여러 번 데이터가 불균형합니다. 즉, class1 인스턴스의 수는 많지만 다른 클래스 인스턴스의 수는 적을 수 있습니다. 따라서 데이터 세트의 모든 인스턴스에 대해 알고리즘을 훈련하고 테스트해야 합니다. 그런 다음 데이터 세트에 대해 언급된 모든 정확도의 평균을 취할 수 있습니다.\n\n1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n\nk_Fold 교차검증은 데이터 셋을 나눈다\n\n2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n\n나눠보자 데이터를 5가지 파트로 우리는 테스트를 위해 1파트를 예약하고 4파트에 대해 알고리즘을 훈련시킬것이다\n\n3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\n각 반복에서 테스트 부분을 변경하고 다른 부분에 대해 알고리즘을 훈련하여 프로세스를 계속합니다. 그런 다음 정확도와 오류를 평균하여 알고리즘의 평균 정확도를 얻습니다.\n\nThis is called K-Fold Cross Validation.\n\n이것은 k-Fold 교차 검증으로 불립니다\n\n4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.\n\n알고리즘은 몇몇 트레이닝 데이터에 대해 과소적합 할것입니다 그리고 때때로 과적합 할것입니다 다른 트레이닝셋에 대한 데이터를 그러므로 교차검증을 통해 우리는 일반화된 모델을 성취할수 있습니다","metadata":{}},{"cell_type":"code","source":"# cross Validation (교차검증 과정)\n\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10,shuffle = True, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:55.800492Z","iopub.execute_input":"2022-07-22T01:30:55.801194Z","iopub.status.idle":"2022-07-22T01:30:59.057806Z","shell.execute_reply.started":"2022-07-22T01:30:55.801144Z","shell.execute_reply":"2022-07-22T01:30:59.056784Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# 각각의 알고리즘에 대한 정확도 시각화","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()\n\n# 모델별로 교차 검증을 한 결과이다 ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:59.059293Z","iopub.execute_input":"2022-07-22T01:30:59.059606Z","iopub.status.idle":"2022-07-22T01:30:59.324402Z","shell.execute_reply.started":"2022-07-22T01:30:59.059579Z","shell.execute_reply":"2022-07-22T01:30:59.323401Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()\n\n# 각각의 알고리즘에 대한 정확도를 바플롯으로 확인","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:59.325494Z","iopub.execute_input":"2022-07-22T01:30:59.326577Z","iopub.status.idle":"2022-07-22T01:30:59.547922Z","shell.execute_reply.started":"2022-07-22T01:30:59.326540Z","shell.execute_reply":"2022-07-22T01:30:59.546587Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"- The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\n\n분류 정확도는 불균형으로 인해 때때로 오해의 소지가 있습니다. 모델이 어디에서 잘못되었는지 또는 모델이 어떤 클래스를 잘못 예측했는지 보여주는 혼동 행렬의 도움으로 요약된 결과를 얻을 수 있습니다.\n\n\naccuracy는 오해의 소지가 있어서 잘 쓰지않는다\n","metadata":{}},{"cell_type":"markdown","source":"## Confusion Matrix\nIt gives the number of correct and incorrect classifications made by the classifier.\n\n","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred =cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt = '2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2, wspace=0.2)\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:30:59.549194Z","iopub.execute_input":"2022-07-22T01:30:59.549531Z","iopub.status.idle":"2022-07-22T01:31:05.225513Z","shell.execute_reply.started":"2022-07-22T01:30:59.549501Z","shell.execute_reply":"2022-07-22T01:31:05.224085Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"- Interpreting Confusion Matrix\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\n왼쪽 대각선은 각 클래스에 대해 수행된 올바른 예측의 수를 나타내고 오른쪽 대각선은 잘못된 예측의 수를 나타냅니다. rbf-SVM의 첫 번째 플롯을 살펴보겠습니다.\n\n1)The no. of correct predictions are 491(for dead) + 247(for survived) with the mean CV accuracy being (491+247)/891 = 82.8% which we did get earlier.\n\n2)Errors--> Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n아니. 정확한 예측은 491(사망) + 247(생존)이며 평균 CV 정확도는 (491+247)/891 = 82.8%입니다.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.\n모든 매트릭스들을 봄으로써, 우리는 rbf-SVM이 더 높은 기회로 정확하게 사망자를 예측했다는 것을 말할 수 있다. 그러나 NaiveBayes는 생존한 승객을 정확하게 예측할 확률이 더 높다고 말할수 있다","metadata":{}},{"cell_type":"markdown","source":"- Hyper-Parameters Tuning\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.   \n머신러닝 모델은 블랙박스와 같다. 우리가 켜거나 더나은 모델을 바꿀수 있는 블랙박스에 몇몇 기계 학습 모델은 블랙박스와 같습니다. 이 Black-Box에 대한 몇 가지 기본 매개변수 값이 있으며 더 나은 모델을 얻기 위해 조정하거나 변경할 수 있습니다. SVM 모델의 C 및 감마와 마찬가지로 다른 분류기에 대한 유사하게 다른 매개변수를 하이퍼 매개변수라고 하며 알고리즘의 학습률을 변경하고 더 나은 모델을 얻기 위해 조정할 수 있습니다. 이것을 하이퍼파라미터 튜닝이라고 합니다.\n\nWe will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.   \n우리는 SVM과 RandomForests와 같은 2개의 최고의 분류기에 대한 하이퍼 매개변수를 조정할 것입니다.","metadata":{}},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:31:05.227270Z","iopub.execute_input":"2022-07-22T01:31:05.228448Z","iopub.status.idle":"2022-07-22T01:31:36.075311Z","shell.execute_reply.started":"2022-07-22T01:31:05.228399Z","shell.execute_reply":"2022-07-22T01:31:36.074003Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)\n\n#이 코드는 RandomForestClassifier의 하이퍼 파라미터 중 n_estimators로 성능 조절을 했다.","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:31:36.076721Z","iopub.execute_input":"2022-07-22T01:31:36.077208Z","iopub.status.idle":"2022-07-22T01:32:28.406869Z","shell.execute_reply.started":"2022-07-22T01:31:36.077168Z","shell.execute_reply":"2022-07-22T01:32:28.405472Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"The best score for Rbf-Svm is 82.82% with C=0.05 and gamma=0.1. For RandomForest, score is abt 81.8% with n_estimators=900.","metadata":{}},{"cell_type":"markdown","source":"# Ensembling\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stability of the model. Ensembling can be done in ways like:\n\n1)Voting Classifier\n\n2)Bagging\n\n3)Boosting.","metadata":{}},{"cell_type":"markdown","source":"## Voting Classifier\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                             ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                             ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                             ('LR',LogisticRegression(C=0.05)),\n                                             ('DT',DecisionTreeClassifier(random_state=0)),\n                                             ('NB',GaussianNB()),\n                                             ('svm',svm.SVC(kernel='linear',probability=True))],\n                                            voting='soft').fit(train_X,train_Y)\n                            \nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y,cv=10,scoring = \"accuracy\")\nprint('The cross validation score is',cross.mean())\n\n# voting Classifier은 여러 다른 머신러닝 모델의 예측을 결합한 것이다\n# 여러 모형에서 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정이다\n","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:32:28.408421Z","iopub.execute_input":"2022-07-22T01:32:28.409025Z","iopub.status.idle":"2022-07-22T01:32:43.624762Z","shell.execute_reply.started":"2022-07-22T01:32:28.408984Z","shell.execute_reply":"2022-07-22T01:32:43.623296Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## Bagging\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n\n- Bagged KNN\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction = model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:', result.mean())\n\n# 분산이 높은 모델에서 잘 작동되는 모델로 배깅은 주어진 자료에서 여러개의 붓스트랩 자료를 생성하고\n# 각 붓스트랩 자료에서 예측모형을 만든 후 결합하여 최종 예측모델을 만든다","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:32:43.626222Z","iopub.execute_input":"2022-07-22T01:32:43.626735Z","iopub.status.idle":"2022-07-22T01:33:10.532067Z","shell.execute_reply.started":"2022-07-22T01:32:43.626696Z","shell.execute_reply":"2022-07-22T01:33:10.530625Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Bagged DecisionTree","metadata":{}},{"cell_type":"code","source":"model=BaggingClassifier(base_estimator = DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:33:10.533619Z","iopub.execute_input":"2022-07-22T01:33:10.534047Z","iopub.status.idle":"2022-07-22T01:33:13.900989Z","shell.execute_reply.started":"2022-07-22T01:33:10.534012Z","shell.execute_reply":"2022-07-22T01:33:13.898343Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## Boosting\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.","metadata":{}},{"cell_type":"markdown","source":"- AdaBoost(Adaptive Boosting)\nThe weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base_estimator to any algorithm of our choice.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:33:13.902501Z","iopub.execute_input":"2022-07-22T01:33:13.903249Z","iopub.status.idle":"2022-07-22T01:33:18.368454Z","shell.execute_reply.started":"2022-07-22T01:33:13.903207Z","shell.execute_reply":"2022-07-22T01:33:18.367182Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"- Stochastic Gradient Boosting\nHere too the weak learner is a Decision Tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad= GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated scor for Gradient Boosting is',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:33:18.370012Z","iopub.execute_input":"2022-07-22T01:33:18.370373Z","iopub.status.idle":"2022-07-22T01:33:24.055745Z","shell.execute_reply.started":"2022-07-22T01:33:18.370341Z","shell.execute_reply":"2022-07-22T01:33:24.054562Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxgboost = xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score fot XGBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:33:24.059622Z","iopub.execute_input":"2022-07-22T01:33:24.060030Z","iopub.status.idle":"2022-07-22T01:33:50.973826Z","shell.execute_reply.started":"2022-07-22T01:33:24.059995Z","shell.execute_reply":"2022-07-22T01:33:50.972802Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"- We got the highest accuracy for AdaBoost. We will try to increase it with Hyper-Parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"### Hyper-Parameter Tuning for AdaBoost","metadata":{}},{"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nhyper = {'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T01:33:50.975305Z","iopub.execute_input":"2022-07-22T01:33:50.975935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The maximum accuracy we can get with AdaBoost is 83.16% with n_estimators=200 and learning_rate=0.05","metadata":{}},{"cell_type":"markdown","source":"- confusion Matrix for the Best Model","metadata":{}},{"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel = AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()\n\n# ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"그래프가 답과 다름 / y축 index가 다름","metadata":{}}]}